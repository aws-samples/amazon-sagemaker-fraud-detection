{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b41072",
   "metadata": {},
   "source": [
    "### Hosting a HuggingFace model in Amazon Sagemaker\n",
    "#### This notebook runs on both GPU and CPU, but it runs faster on a GPU since it has an NLP nerual network model in it and it is used twice prior to hosting: to generate word embeddings and to test the model locally. Measured end-2-end runtimes are: ml.p3.2xlarge: 9.5 minutes. ml.c5.4xlarge: 18.5 minutes. Your mileage may vary.\n",
    "#### This notebook pulls a BERT model from HuggingFace repo to demonstrate a Semantic Search use-case.  \n",
    "#### In addition, this notebook demonstrates the following capabilities:\n",
    "* how to add extra artifacts to the pretrained model.tar.gz, such as word embeddings, for example. \n",
    "* how to pull a container from ECR.\n",
    "* how to add custom inference.py “entry point” script, as well as additional files/directories to be used during inference process.\n",
    "* how to add dependencies (eg requirements.txt) to be executed at upon launching the container.\n",
    "* how to set up environment variables to allow the code inside inference container to access content outside of VPC. for example. \n",
    "* how to deploy the model \n",
    "* how to invoke the model.\n",
    "* how to call CloudWatch via Python APIs to obtain model invocation metrics such as model and overhead latency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a577b",
   "metadata": {},
   "source": [
    "![semantic_search_image](notebook_images/semantic_search_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8e153",
   "metadata": {},
   "source": [
    "#### Semantic Search is a searching technique which incorporates contextual meaning of words or phrases. It relies on offline convertion of the corpus of data into word embeddings using (in this example) a distilbert model. During a real-time search (i.e. \"model inference\"), a query is also converted into an embedding using the same distilbert model. Query_embedding is then ranked against all embeddings of the corpus using cosine similarity and the top-ranked matches are presented . In this simplified example, corpus embeddings are included in the same tarball as the model itself. Including the corpus in the model's tarball file is obviously is not a scalable solution as it is limited by the available memory and compute power of the inference endpoint. A scalable solution would involve using Elastic Search instead.\n",
    "#### See this reference for the semantic search similarity algorithm implemented here:\n",
    "https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "\n",
    "#### For the dataset, we will use misinformation_papers.csv file available here: \n",
    "https://github.com/orion-search/tutorials/blob/master/data/misinformation_papers.csv\n",
    "Please, use your own method to download this dataset and place it in the ./dataset folder.\n",
    "\n",
    "Publications and other sources used in creating this notebook: https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8 \n",
    "\n",
    "https://github.com/orion-search/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499accbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "nb_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b475066",
   "metadata": {},
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b190c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.3.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a614c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker>=2.48.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (2.119.0)\n",
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.121.2.tar.gz (620 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m621.0/621.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.12.3\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets[s3]>=1.18.3\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (21.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.20 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (1.26.22)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (1.21.2)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (3.19.4)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (4.8.2)\n",
      "Collecting packaging==20.9\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m648.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (1.3.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (0.2.8)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sagemaker>=2.48.0) (0.7.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging==20.9->sagemaker>=2.48.0) (3.0.6)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers>=4.12.3) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers>=4.12.3) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers>=4.12.3) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers>=4.12.3) (3.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers>=4.12.3) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (2021.11.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (7.0.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (0.3.4)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (1.24.19)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from datasets[s3]>=1.18.3) (0.4.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.26.20->sagemaker>=2.48.0) (0.10.0)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.29.29-py3-none-any.whl (10.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from boto3<2.0,>=1.26.20->sagemaker>=2.48.0) (0.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore->datasets[s3]>=1.18.3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from botocore->datasets[s3]>=1.18.3) (1.26.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.12.3) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.48.0) (3.6.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers>=4.12.3) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers>=4.12.3) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers>=4.12.3) (2021.10.8)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.3) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.3) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.3) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.3) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->datasets[s3]>=1.18.3) (5.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pandas->sagemaker>=2.48.0) (2021.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pathos->sagemaker>=2.48.0) (0.3.0)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from schema->sagemaker>=2.48.0) (21.6.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.121.2-py2.py3-none-any.whl size=844051 sha256=737bbdbc083e9d4d5bc591b8e149e92cd7320b103ce555d3b2be717c92cc20bc\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/46/dc/fc/d947addd83079e53244196ead545f1800abfc5e043a0cf9c1a\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: tokenizers, xxhash, packaging, responses, huggingface-hub, botocore, transformers, datasets, sagemaker\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.24.19\n",
      "    Uninstalling botocore-1.24.19:\n",
      "      Successfully uninstalled botocore-1.24.19\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.119.0\n",
      "    Uninstalling sagemaker-2.119.0:\n",
      "      Successfully uninstalled sagemaker-2.119.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "shap 0.40.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
      "awscli 1.27.22 requires botocore==1.29.22, but you have botocore 1.29.29 which is incompatible.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.29.29 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.29.29 datasets-2.7.1 huggingface-hub-0.11.1 packaging-20.9 responses-0.18.0 sagemaker-2.121.2 tokenizers-0.13.2 transformers-4.25.1 xxhash-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers>=4.12.3\" \"datasets[s3]>=1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d881769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (4.25.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (0.11.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (1.21.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (3.6.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from sentence-transformers) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from torchvision->sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.10.8)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=707885de39bfa61f853975b98d8725a82be2bd760cad3fe0bf0e35fe06743bc5\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/30/b4/1c/7509ecb4c391a7be4cdf2ff04df077a568cd52471007e436e6\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d062c3",
   "metadata": {},
   "source": [
    "### Importing libraries, setting up AWS S3 buckets and AWS IAM roles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3212efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::328296961357:role/service-role/AmazonSageMaker-ExecutionRole-20191125T182032\n",
      "sagemaker bucket: huggingface-bucket-se\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it is not provided. \n",
    "# however, typically enterprise users don't have permissions to create their own buckets or \n",
    "# have AWS servcies (such as SageMaker) create them for you. \n",
    "sagemaker_session_bucket= \"huggingface-bucket-se\"\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sm_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sm_session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "default_bucket=sagemaker_session_bucket\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sm_session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sm_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629df750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import sagemaker\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "from sentence_transformers import util, losses\n",
    "import boto3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aaab9",
   "metadata": {},
   "source": [
    "## 1) Download and save a model from HuggingFace that would generate word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65b99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to pull quora_distilbert_model from Hugging Face repo.\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets\n",
    "def quora_distilbert_model():\n",
    "    # Load quora-distilbert-base\n",
    "    word_emb = models.Transformer('sentence-transformers/quora-distilbert-base')\n",
    "    pooling = models.Pooling(word_emb.get_word_embedding_dimension())\n",
    "    model = SentenceTransformer(modules=[word_emb, pooling])\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, 'quora_distilbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16a6af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd0096cfcce4243beecad2a3cd22d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/540 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f8474747c4d03b6f73c9fd48961b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2743aa7758344af689dc655e44c71faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/490 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00cb1ab38684579973166df8887d207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a299fb290c140d4ad4f176bb83eb759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3189639806942cfacce15954ec8c354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_backward_hooks': OrderedDict(),\n",
      " '_buffers': OrderedDict(),\n",
      " '_forward_hooks': OrderedDict(),\n",
      " '_forward_pre_hooks': OrderedDict(),\n",
      " '_is_full_backward_hook': None,\n",
      " '_load_state_dict_pre_hooks': OrderedDict(),\n",
      " '_model_card_text': None,\n",
      " '_model_card_vars': {},\n",
      " '_model_config': {},\n",
      " '_modules': OrderedDict([('0',\n",
      "                           Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel ),\n",
      "                          ('1',\n",
      "                           Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}))]),\n",
      " '_non_persistent_buffers_set': set(),\n",
      " '_parameters': OrderedDict(),\n",
      " '_state_dict_hooks': OrderedDict(),\n",
      " '_target_device': device(type='cpu'),\n",
      " 'training': True}\n"
     ]
    }
   ],
   "source": [
    "#saving the model on local \"disk\"\n",
    "distilbert_model=quora_distilbert_model()[0]\n",
    "from pprint import pprint\n",
    "pprint(vars(distilbert_model))\n",
    "distilbert_model.save('./trained_models/quora-distilbert-untrained/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b5793",
   "metadata": {},
   "source": [
    "### 2) Generate embeddings file and compress it as *.pkl format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3d2ff",
   "metadata": {},
   "source": [
    "#### Depending on the gype (GPU vs CPU) and the size of the machine where you notebook is running, generating the embeddings may take a some time. For example, the below \"Generate Embeddings\" cell takes 6 min on ml.c5.4xlarge (16 vCPU, 32GiB memory), 12 min on  ml.c5.2xlarge (8 vCPU, 16GiB memory) and 40 seconds on p3.2xlarge (1 V100 GPU/16 GiB mem; 8 vCPU/61GiB memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b04e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Embeddings\n",
    "docs = set()\n",
    "with open('./dataset/misinformation_papers.csv') as fIn:\n",
    "    for line in fIn:\n",
    "        doc = line.rstrip(\"\\n\")\n",
    "        docs.add(doc)\n",
    "\n",
    "docs = list(docs)        \n",
    "paragraph_emb = distilbert_model.encode([d for d in docs], convert_to_tensor=True)    \n",
    "\n",
    "# Save Embeddings as a pickle file on local \"disk\"\n",
    "with open('./inference/embed_support_titles.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'titles': docs, 'embeddings': paragraph_emb}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c644e9",
   "metadata": {},
   "source": [
    "### 3) Test Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f9bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load sentences & embeddings from disc\n",
    "with open('./inference/embed_support_titles.pkl', \"rb\") as fIn:    \n",
    "    stored_data = pickle.load(fIn)\n",
    "    support_titles = stored_data['titles']\n",
    "    support_titles_embed = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebccd4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"score\": 0.7601248025894165, \"title\": \"Those difficulties tend to lead students to make errors in building English\"}, {\"score\": 0.7531709671020508, \"title\": \"developmental stage to gain English competence and errors are a result from the \"}, {\"score\": 0.7259856462478638, \"title\": \" grammar intensively so they often produce errors regarding to grammatical rules in\"}, {\"score\": 0.6749558448791504, \"title\": \"students often feel difficult in learning English especially in terms of grammar.\"}, {\"score\": 0.6726627945899963, \"title\": \"interlanguage. This study aims to find out the grammatical errors that students\"}, {\"score\": 0.6695536971092224, \"title\": \" grammar items which relates to the errors that mostly produced in narrative text such\"}, {\"score\": 0.6657508015632629, \"title\": \"AN ANALYSIS OF STUDENTS\\u2019 ERRORS IN USING ENGLISH PRONOUNS: A CASE STUDY AT NINTH GRADE STUDENTS OF SMPN 2 LINGSAR IN ACADEMIC YEAR 2017/2018,\\\"This study is aimed to analyze students\\u2019 errors in using English pronouns functioning as; \"}, {\"score\": 0.6607153415679932, \"title\": \"INTERLANGUAGE: GRAMMATICAL ERRORS (A Case Study of First Yein the Academic Year 2014/2015)(A Case Study of First Year of MAN 2 Banjarnegarahe Academic Year 2014/2015),\\\"The difference between Indonesian and English language makes the \"}, {\"score\": 0.6445220112800598, \"title\": \"And then there\\u2019s the problem of language. George Orwell wrote: \\u2018English is a beautiful language, but it can be corrupted and made ugly by foolish ideas and, having been corrupted, it can only express more foolishness\\u20262\\u2019 \"}, {\"score\": 0.6419546604156494, \"title\": \"tongue or it is called interlingual errors.\\\",2017,0,2927372470,1\"}]\n"
     ]
    }
   ],
   "source": [
    "query = \"grammatical errors in spoken English\"\n",
    "query_emb = distilbert_model.encode(query, convert_to_tensor=True)\n",
    "prediction = util.semantic_search(query_emb, support_titles_embed, top_k=10)[0]\n",
    "\n",
    "output = []\n",
    "for hit in prediction:\n",
    "    doc = support_titles[hit['corpus_id']]\n",
    "    output.append({\"score\": hit['score'], \"title\": doc})\n",
    "\n",
    "print(json.dumps(output))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c37c678",
   "metadata": {},
   "source": [
    "# Sample expected output:\n",
    "\n",
    "[{\"score\": 0.7601252198219299, \"title\": \"Those difficulties tend to lead students to make errors in building English\"}, {\"score\": 0.7531715035438538, \"title\": \"developmental stage to gain English competence and errors are a result from the \"}, {\"score\": 0.7259856462478638, \"title\": \" grammar intensively so they often produce errors regarding to grammatical rules in\"}, {\"score\": 0.6749561429023743, \"title\": \"students often feel difficult in learning English especially in terms of grammar.\"}, {\"score\": 0.6726628541946411, \"title\": \"interlanguage. This study aims to find out the grammatical errors that students\"}, {\"score\": 0.6695539355278015, \"title\": \" grammar items which relates to the errors that mostly produced in narrative text such\"}, {\"score\": 0.6657513380050659, \"title\": \"AN ANALYSIS OF STUDENTS\\u2019 ERRORS IN USING ENGLISH PRONOUNS: A CASE STUDY AT NINTH GRADE STUDENTS OF SMPN 2 LINGSAR IN ACADEMIC YEAR 2017/2018,\\\"This study is aimed to analyze students\\u2019 errors in using English pronouns functioning as; \"}, {\"score\": 0.6607156991958618, \"title\": \"INTERLANGUAGE: GRAMMATICAL ERRORS (A Case Study of First Yein the Academic Year 2014/2015)(A Case Study of First Year of MAN 2 Banjarnegarahe Academic Year 2014/2015),\\\"The difference between Indonesian and English language makes the \"}, {\"score\": 0.644521951675415, \"title\": \"And then there\\u2019s the problem of language. George Orwell wrote: \\u2018English is a beautiful language, but it can be corrupted and made ugly by foolish ideas and, having been corrupted, it can only express more foolishness\\u20262\\u2019 \"}, {\"score\": 0.6419550180435181, \"title\": \"tongue or it is called interlingual errors.\\\",2017,0,2927372470,1\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb5edf",
   "metadata": {},
   "source": [
    "### 4) Prepare model file to be deployed on a SageMaker Inference Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8546b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Embeddings from inference_extra/ -> to the top-level of the HuggingFace model\n",
    "# Note, SageMaker documentation can be interpreted as saying that the embeddings *.pkl file could be placed in \n",
    "# ./inference/ directory and referenced in inference.py via ./inference/filename.pkl. This is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e0b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp ./inference/embed_support_titles.pkl ./trained_models/quora-distilbert-untrained/embed_support_titles.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec50270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm ./inference/embed_support_titles.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be57e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trained_models/quora-distilbert_untrained_model_artifact.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# create a model *.tar.gz file\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.sep)\n",
    "\n",
    "source_dir=\"./trained_models/quora-distilbert-untrained\"\n",
    "output_filename=\"./trained_models/quora-distilbert_untrained_model_artifact.tar.gz\"\n",
    "make_tarfile(output_filename, source_dir)\n",
    "print(output_filename)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c1a22b8",
   "metadata": {},
   "source": [
    "#upload model file with embeddings to S3\n",
    "import sagemaker\n",
    "\n",
    "paths = [output_filename]\n",
    "key_prefix='datalab-hf-1/trained_models'\n",
    "\n",
    "sesh = sagemaker.Session()\n",
    "\n",
    "for path in paths:\n",
    "    s3_model_uri = sesh.upload_data(bucket=sagemaker_session_bucket, \n",
    "                                                      path=path, \n",
    "                                                      key_prefix=key_prefix)\n",
    "print(s3_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa9b6803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://huggingface-bucket-se/datalab-hf-1/trained_models/121311PM\n",
      "s3://huggingface-bucket-se/datalab-hf-1/trained_models/121311PM/quora-distilbert_untrained_model_artifact.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#upload model file with embeddings to S3\n",
    "from datetime import datetime\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "key_prefix='datalab-hf-1/trained_models'\n",
    "model_s3_uri_prefix=os.path.join(\"s3://\", default_bucket, key_prefix, datetime.now().strftime(\"%m%d%I%p\"))\n",
    "s3_model_uri=S3Uploader.upload(desired_s3_uri = model_s3_uri_prefix,\n",
    "                                   local_path = output_filename,\n",
    "                                   sagemaker_session=sm_session)\n",
    "print(model_s3_uri_prefix)\n",
    "print(s3_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3968aca",
   "metadata": {},
   "source": [
    "### 5) Create SageMaker Endpoint with HuggingFace Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abae3b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:1.9-transformers4.12-cpu-py38-ubuntu20.04'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Retrieve huggingface docker image for the inference container\n",
    "hf_inf_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"huggingface\",\n",
    "    region=\"us-west-2\",\n",
    "    version='4.12',\n",
    "    py_version=\"py38\",\n",
    "    image_scope=\"inference\",\n",
    "    base_framework_version='pytorch1.9',\n",
    "    instance_type=\"ml.c5.xlarge\" #\"ml.p3.2xlarge\",\n",
    ")\n",
    "\n",
    "hf_inf_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab16260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "source_dir=\"./inference\" \n",
    "\n",
    "#kms_key = \"00f78e2f-dd0b-488b-aa5f-cf0f39cb374f\"\n",
    "\n",
    "#env_variables_dict = {\n",
    "#    \"SAGEMAKER_TS_BATCH_SIZE\": \"3\",\n",
    "#    \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"100000\",\n",
    "#    \"SAGEMAKER_TS_MIN_WORKERS\": \"1\",\n",
    "#    \"SAGEMAKER_TS_MAX_WORKERS\": \"1\",\n",
    "#    'http_proxy': proxy_endpoint, \n",
    "#    'HTTP_PROXY': proxy_endpoint,\n",
    "#    'https_proxy': proxy_endpoint,\n",
    "#    'HTTPS_PROXY': proxy_endpoint,\n",
    "#    'NO_PROXY': no_proxy_endpoint,\n",
    "#    'no_proxy': no_proxy_endpoint\n",
    "#}\n",
    "\n",
    "hugface_model = HuggingFaceModel(\n",
    "    model_data=s3_model_uri,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    image_uri=hf_inf_image_uri,\n",
    "    source_dir=source_dir,\n",
    "    entry_point=\"inference.py\",\n",
    "    dependencies=[\"./inference/requirements.txt\"], \n",
    "#    env=env_variables_dict,\n",
    "#    vpc_config = {'Subnets': ['subnet-XXX', 'subnet-XXX'], 'SecurityGroupIds': ['sg-XXX', 'sg-XXX', 'sg-XXX']},\n",
    "#    model_kms_key = kms_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "654949e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!289.9066252708435\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "inf_instance_type = \"ml.c5.xlarge\" #\"ml.p3.2xlarge\"\n",
    "predictor = hugface_model.deploy(\n",
    "#    kms_key=kms_key,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inf_instance_type,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "toc=time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ae7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-inference-2022-12-13-23-46-59-900'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56e5b6",
   "metadata": {},
   "source": [
    "### 4) Test Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9baa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '0333121d-02c5-423e-97dd-86293f248f42', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0333121d-02c5-423e-97dd-86293f248f42', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Tue, 13 Dec 2022 23:51:31 GMT', 'content-type': 'application/json', 'content-length': '1551'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7fa0fa5e13a0>}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.762311</td>\n",
       "      <td>developmental stage to gain English competence and errors are a result from the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.740182</td>\n",
       "      <td>Those difficulties tend to lead students to make errors in building English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.733867</td>\n",
       "      <td>grammar intensively so they often produce errors regarding to grammatical rules in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.709804</td>\n",
       "      <td>interlanguage. This study aims to find out the grammatical errors that students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.709719</td>\n",
       "      <td>INTERLANGUAGE: GRAMMATICAL ERRORS (A Case Study of First Yein the Academic Year 2014/2015)(A Case Study of First Year of MAN 2 Banjarnegarahe Academic Year 2014/2015),\"The difference between Indonesian and English language makes the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.703969</td>\n",
       "      <td>sentences. However, errors are actually natural because they are regarded as a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.701805</td>\n",
       "      <td>grammar items which relates to the errors that mostly produced in narrative text such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.693199</td>\n",
       "      <td>AN ANALYSIS OF STUDENTS’ ERRORS IN USING ENGLISH PRONOUNS: A CASE STUDY AT NINTH GRADE STUDENTS OF SMPN 2 LINGSAR IN ACADEMIC YEAR 2017/2018,\"This study is aimed to analyze students’ errors in using English pronouns functioning as;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.653287</td>\n",
       "      <td>students often feel difficult in learning English especially in terms of grammar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.652491</td>\n",
       "      <td>•\\tThe Harms of Social Spoiling, Social Construction, and Language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  \\\n",
       "0  0.762311   \n",
       "1  0.740182   \n",
       "2  0.733867   \n",
       "3  0.709804   \n",
       "4  0.709719   \n",
       "5  0.703969   \n",
       "6  0.701805   \n",
       "7  0.693199   \n",
       "8  0.653287   \n",
       "9  0.652491   \n",
       "\n",
       "                                                                                                                                                                                                                                       title  \n",
       "0                                                                                                                                                           developmental stage to gain English competence and errors are a result from the   \n",
       "1                                                                                                                                                                Those difficulties tend to lead students to make errors in building English  \n",
       "2                                                                                                                                                         grammar intensively so they often produce errors regarding to grammatical rules in  \n",
       "3                                                                                                                                                            interlanguage. This study aims to find out the grammatical errors that students  \n",
       "4  INTERLANGUAGE: GRAMMATICAL ERRORS (A Case Study of First Yein the Academic Year 2014/2015)(A Case Study of First Year of MAN 2 Banjarnegarahe Academic Year 2014/2015),\"The difference between Indonesian and English language makes the   \n",
       "5                                                                                                                                                            sentences. However, errors are actually natural because they are regarded as a   \n",
       "6                                                                                                                                                      grammar items which relates to the errors that mostly produced in narrative text such  \n",
       "7   AN ANALYSIS OF STUDENTS’ ERRORS IN USING ENGLISH PRONOUNS: A CASE STUDY AT NINTH GRADE STUDENTS OF SMPN 2 LINGSAR IN ACADEMIC YEAR 2017/2018,\"This study is aimed to analyze students’ errors in using English pronouns functioning as;   \n",
       "8                                                                                                                                                          students often feel difficult in learning English especially in terms of grammar.  \n",
       "9                                                                                                                                                                         •\\tThe Harms of Social Spoiling, Social Construction, and Language  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-test-endpoints.html\n",
    "\n",
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "# If ContentType=\"application/json\", input must be a list of 1 string\n",
    "query = '[\"grammatical errors in spoken English\"]'\n",
    "\n",
    "# Invoke the endpoint using the client created earlier\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "                            EndpointName=endpoint_name, \n",
    "                            Body=query.encode('utf-8'),\n",
    "                            ContentType=\"application/json\", \n",
    "                            Accept=\"application/json\",\n",
    "                            )\n",
    "\n",
    "# Optional - Print the response body and decode it so it is human read-able.\n",
    "print(response)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Pandas DataFrame from lists of dicts. \n",
    "list_of_dicts_output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "df_output = pd.DataFrame(list_of_dicts_output)\n",
    "    \n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c4f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up to save cost\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98257769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook execution time, seconds: 1179.633066892624\n"
     ]
    }
   ],
   "source": [
    "nb_end = time.time()\n",
    "print(f\"notebook execution time, seconds: {nb_end-nb_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd20425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
